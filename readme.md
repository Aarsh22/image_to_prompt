# Image to Prompt


A generative text-to-image model is a model that can generate an image from a text prompt.

This repository is a final project for the course [EECM30064 Deep Learning](https://timetable.nycu.edu.tw/?r=main/crsoutline&Acy=111&Sem=2&CrsNo=535361&lang=zh-tw)

## Motivation and Background

[Stable Diffusion - Image to Prompts](https://www.kaggle.com/competitions/stable-diffusion-image-to-prompts/overview) is a competition on Kaggle.

The goal of this competition is to reverse the typical direction of a generative text-to-image model: instead of generating an image from a text prompt.

We want to  create a model which can predict the text prompt given a generated image. And making predictions on a dataset containing a wide variety of $\verb|(prompt, image)|$ pairs generated by Stable Diffusion 2.0, in order to understand how reversible the latent relationship is.

Sample images from the competition dataset and their corresponding prompts are shown below.

<table>
    <tr>
        <th><center>Image</center></th>
        <th><center>Prompt</center></th>
    </tr>
    <tr>
        <td><center><img src="./images/92e911621.png" width="200" height="200"></center></td>
        <td><center><code>digital illustration of a blue dinosaur with a piece of cheese in its mouth walking through a forest</code></center></td>
    </tr>
    <tr>
        <td><center><img src="./images/227ef0887.png" width="200" height="200"></center></td>
        <td><center><code>a close up wood carving shoeing the texture of wood</code></center></td>
    </tr>
    <tr>
        <td><center><img src="./images/20057f34d.png" width="200" height="200"></center></td>
        <td><center><code>digital art selected for the #</code></center></td>
    </tr>
    <tr>
        <td><center><img src="./images/a4e1c55a9.png" width="200" height="200"></center></td>
        <td><center><code>a drawing of robot</code></center></td>
    </tr>
    <tr>
        <td><center><img src="./images/c98f79f71.png" width="200" height="200"></center></td>
        <td><center><code>a painting of a roman man with a lizard crawling over his shoulder</code></center></td>
    </tr>
    <tr>
        <td><center><img src="./images/d8edf2e40.png" width="200" height="200"></center></td>
        <td><center><code>astronaut walking on a road lined with cherry blossom trees</code></center></td>
    </tr>
    <tr>
        <td><center><img src="./images/f27825b2c.png" width="200" height="200"></center></td>
        <td><center><code>a man in a blue tshirt and khakhi pants stands in front of a bakery counter</code></center></td>
    </tr>
</table>

## Methodology

Our method is to ensemble the CLIP Interrogator, OFA model, and ViT model.

Here's the ratio for three different model
- Vision Transformer (ViT) model:  74.88%
- CLIP Interrogator: 21.12%
- OFA model fine-tuned for image captioning: 4%

## Application and Datasets



## References

[1] [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020.pdf)

[2] [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929.pdf)

[3] [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/pdf/1409.1556.pdf)

[4] [SentenceTransformers](https://www.sbert.net/)

[5] [CLIPInterrogator + OFA + ViT](https://www.kaggle.com/code/motono0223/clipinterrogator-ofa-vit)

[6] [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/pdf/2103.14030.pdf)

[7] [CoCa: Contrastive Captioners are Image-Text Foundation Models](https://arxiv.org/pdf/2205.01917.pdf)
